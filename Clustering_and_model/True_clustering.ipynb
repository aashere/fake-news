{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQwzTQWo-0hL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR_EC6WbeDKa"
   },
   "outputs": [],
   "source": [
    "\n",
    "stop_word_dic={}\n",
    "\n",
    "with open('sample_data/extra_stop.txt','r') as file:\n",
    "   \n",
    "    # reading each line    \n",
    "    for line in file:\n",
    "   \n",
    "        # reading each word        \n",
    "        for word in line.split():\n",
    "   \n",
    "            # displaying the words           \n",
    "            stop_word_dic[word]=word\n",
    "with open('sample_data/stop_words_english.txt','r') as file:\n",
    "   \n",
    "    # reading each line    \n",
    "    for line in file:\n",
    "   \n",
    "        # reading each word        \n",
    "        for word in line.split():\n",
    "   \n",
    "            # displaying the words           \n",
    "            stop_word_dic[word]=word\n",
    "\n",
    "\n",
    "\n",
    "closed_class_stop_words = ['a', 'the', 'an', 'and', 'or', 'but', 'about', 'above', 'after', 'along', 'amid', 'among', \\\n",
    "                           'as', 'at', 'by', 'for', 'from', 'in', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', \\\n",
    "                           'onto', 'out', 'over', 'past', 'per', 'plus', 'since', 'till', 'to', 'under', 'until', 'up', \\\n",
    "                           'via', 'vs', 'with', 'that', 'can', 'cannot', 'could', 'may', 'might', 'must', \\\n",
    "                           'need', 'ought', 'shall', 'should', 'will', 'would', 'have', 'had', 'has', 'having', 'be', \\\n",
    "                           'is', 'am', 'are', 'was', 'were', 'being', 'been', 'get', 'gets', 'got', 'gotten', \\\n",
    "                           'getting', 'seem', 'seeming', 'seems', 'seemed', \\\n",
    "                           'enough', 'both', 'all', 'your' 'those', 'this', 'these', \\\n",
    "                           'their', 'the', 'that', 'some', 'our', 'no', 'neither', 'my', \\\n",
    "                           'its', 'his' 'her', 'every', 'either', 'each', 'any', 'another', \\\n",
    "                           'an', 'a', 'just', 'mere', 'such', 'merely' 'right', 'no', 'not', \\\n",
    "                           'only', 'sheer', 'even', 'especially', 'namely', 'as', 'more', \\\n",
    "                           'most', 'less' 'least', 'so', 'enough', 'too', 'pretty', 'quite', \\\n",
    "                           'rather', 'somewhat', 'sufficiently' 'same', 'different', 'such', \\\n",
    "                           'when', 'why', 'where', 'how', 'what', 'who', 'whom', 'which', \\\n",
    "                           'whether', 'why', 'whose', 'if', 'anybody', 'anyone', 'anyplace', \\\n",
    "                           'anything', 'anytime' 'anywhere', 'everybody', 'everyday', \\\n",
    "                           'everyone', 'everyplace', 'everything' 'everywhere', 'whatever', \\\n",
    "                           'whenever', 'whereever', 'whichever', 'whoever', 'whomever' 'he', \\\n",
    "                           'him', 'his', 'her', 'she', 'it', 'they', 'them', 'its', 'their', 'theirs', \\\n",
    "                           'you', 'your', 'yours', 'me', 'my', 'mine', 'I', 'we', 'us', 'much', 'and/or',\"wa\",\"ha\",\",\",\"n't\" ,\"”\" ,\"“\" ,\"’\"\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqHxRcH4Bzk4"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_true= pd.read_csv(\"sample_data/true_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT2dvk2SIPho"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuX04x_NJwUx"
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_dic():\n",
    "  for stop in closed_class_stop_words:\n",
    "    stop_word_dic[stop]=stop\n",
    "            \n",
    "make_dic()  \n",
    "\n",
    "def clean_me(string):\n",
    "  return remove_stop_words(word_tokenize(string))\n",
    "\n",
    "def remove_stop_words(text):\n",
    "  result=[]\n",
    "  for word in text:\n",
    "    word=word.lower()\n",
    "    word=lemmatizer.lemmatize(word)\n",
    "    if word in stop_word_dic:\n",
    "      continue\n",
    "    else:\n",
    "      result.append(word)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-6kdDsFkoB9"
   },
   "outputs": [],
   "source": [
    "def clean_data_frameTF_IDF(df,col=\"Statement\"):\n",
    "  text=df[col]\n",
    "  clean_text=[]\n",
    "  id=[]\n",
    "  Link=[]\n",
    "  df_clean=pd.DataFrame()\n",
    "  for index, row in df.iterrows():\n",
    "    text=row[col]\n",
    "    res=\" \".join(clean_me(text))\n",
    "    clean_text.append(res)\n",
    "    if col ==\"Statement\":\n",
    "      id.append(row['Date'])\n",
    "      Link.append(row['Link'])\n",
    "\n",
    "\n",
    "  if col ==\"Statement\":\n",
    "    #, \"link\":Link\n",
    "    return pd.DataFrame({'Date': id, 'Clean_text': clean_text}),clean_text\n",
    "  else:\n",
    "    return pd.DataFrame({'Clean_tweet': clean_text})\n",
    "\n",
    "df_clean_true=clean_data_frameTF_IDF(df_true)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9EaZ3DNtojU"
   },
   "outputs": [],
   "source": [
    "def clean_data_frame(df,AP,col=\"Statement\"):\n",
    "  text=df[col]\n",
    "  clean_text=[]\n",
    "  id=[]\n",
    "  Link=[]\n",
    "  df_clean=pd.DataFrame()\n",
    "  for index, row in df.iterrows():\n",
    "    text=row[col]\n",
    "  #text = re.sub(r'@.+', text)\n",
    "    res=clean_me(text)\n",
    "    clean_text.append(res)\n",
    "    if col ==\"Statement\":\n",
    "      id.append(row['Date'])\n",
    "      Link.append(row['Link'])\n",
    "  \n",
    "  #clean_text.append(clean_me(row[' text']))\n",
    "  if(AP):\n",
    "    for index, row in df_pants.iterrows():\n",
    "      text=row[col]\n",
    "    #text = re.sub(r'@.+', text)\n",
    "      res=clean_me(text)\n",
    "      clean_text.append(res)\n",
    "      if col ==\"Statement\":\n",
    "        id.append(row['Date'])\n",
    "        Link.append(row['Link'])\n",
    "\n",
    "  if col ==\"Statement\":\n",
    "    return pd.DataFrame({'Date': id, 'Clean_text': clean_text, \"link\":Link})\n",
    "  else:\n",
    "    return pd.DataFrame({'Clean_tweet': clean_text})\n",
    "\n",
    "df_clean_true_list=clean_data_frame(df_true,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9v2VFR6mFZb"
   },
   "outputs": [],
   "source": [
    "corpus=df_clean_true[1]\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3Z6zjccchOr"
   },
   "outputs": [],
   "source": [
    "\n",
    "clustering_true = DBSCAN(eps=0.4, min_samples=6, metric='cosine').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2t_2i5iutfmU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.649899521758E12,
     "user_tz": 240.0,
     "elapsed": 8937.0,
     "user": {
      "displayName": "Avery Greenberg",
      "userId": "01644322353595564422"
     }
    },
    "outputId": "4536e345-7b4b-438e-ecd8-b20db5639cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 2460}\n",
      "printing\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "df=df_clean_true[0]\n",
    "df=pd.concat([df,df[\"Date\"].str.split(expand=True)], axis=1).rename(columns={0:\"month\",1:\"day\",2:'year'})\n",
    "unique, counts = np.unique(clustering_true.labels_, return_counts=True)\n",
    "counts_cluster=(dict(zip(unique, counts)))\n",
    "sorted_dict = {}\n",
    "sorted_keys = sorted(counts_cluster, key=counts_cluster.get, reverse=True) \n",
    "for w in sorted_keys:\n",
    "    sorted_dict[w] = counts_cluster[w]\n",
    "pd.options.display.max_colwidth = 200\n",
    "print(sorted_dict)\n",
    "breaker=0\n",
    "str_result=[]\n",
    "d=df\n",
    "p=False\n",
    "dic_date={}\n",
    "for i,x in enumerate(clustering_true.labels_):\n",
    "  if(x==-1):\n",
    "    d=df.iloc[[i]]\n",
    "    if(len(d.loc[d[\"year\"].str.contains(\":\")])==1):\n",
    "      continue\n",
    "    date=int(d[\"year\"])\n",
    "    Sep = d.loc[(d['Clean_text'].str.contains('year'))]\n",
    "    if(date in dic_date):\n",
    "      if((len(Sep[\"Clean_text\"])!=0) or p):\n",
    "        dic_date[date].append(df_clean_true_list.iloc[i]['Clean_text'])\n",
    "        if(date==2022):\n",
    "          if(str_date in date_count):\n",
    "            date_count[str_date]+=1\n",
    "          else:\n",
    "            date_count[str_date]=1\n",
    "    else:\n",
    "      if(len(Sep[\"Clean_text\"])!=0 or p):\n",
    "        dic_date[date]=[(df_clean_true_list.iloc[i]['Clean_text'])]\n",
    "        if(date==2022):\n",
    "           if(str_date in date_count):\n",
    "            date_count[str_date]+=1\n",
    "           else:\n",
    "            date_count[str_date]=1\n",
    "    Sep = d.loc[(d['Clean_text'].str.contains('acc'))]\n",
    "    if(len(Sep[\"Clean_text\"])!=0):\n",
    "      pass\n",
    "    breaker+=1\n",
    "    \n",
    "    if(breaker>900):\n",
    "      break\n",
    "print(\"printing\")\n",
    "for k,v in date_count.items():\n",
    "  if (k[0:3]==\"Dec\"):\n",
    "    print(k,v)\n",
    "   \n",
    "     \n",
    "print(\"printing\")\n",
    "print(str_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6lGXeBReIws",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.649899525447E12,
     "user_tz": 240.0,
     "elapsed": 166.0,
     "user": {
      "displayName": "Avery Greenberg",
      "userId": "01644322353595564422"
     }
    },
    "outputId": "7fe723de-8c05-4841-df29-92dc58e18721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 4\n",
      "2021 5\n",
      "2020 11\n",
      "2019 15\n",
      "2018 15\n",
      "2017 15\n",
      "2016 26\n",
      "2015 24\n",
      "2014 30\n",
      "2013 27\n",
      "2012 50\n",
      "2011 32\n",
      "2010 32\n",
      "2009 7\n",
      "2008 15\n",
      "2007 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in dic_date.items():\n",
    "    print (k, len(list(filter(None, v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fA4m5KdfffhJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.649899421105E12,
     "user_tz": 240.0,
     "elapsed": 117.0,
     "user": {
      "displayName": "Avery Greenberg",
      "userId": "01644322353595564422"
     }
    },
    "outputId": "f3a5620c-dbc4-4edb-9282-53dea8d46f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 large fully vaccinated teenager 2 large fully vaccinated five- 11- year-olds 17 time hospital vaccinated 20 time die\n",
      "{'vaccinated': 3, 'large': 2, 'fully': 2, 'time': 2, '1': 1, 'teenager': 1, '2': 1, 'five-': 1, '11-': 1, 'year-olds': 1, '17': 1, 'hospital': 1, '20': 1, 'die': 1}\n",
      "{'large_fully': 2, 'fully_vaccinated': 2, '_': 1, '_1': 1, '1_large': 1, 'vaccinated_teenager': 1, 'teenager_2': 1, '2_large': 1, 'vaccinated_five-': 1, 'five-_11-': 1, '11-_year-olds': 1, 'year-olds_17': 1, '17_time': 1, 'time_hospital': 1, 'hospital_vaccinated': 1, 'vaccinated_20': 1, '20_time': 1, 'time_die': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag={}\n",
    "\n",
    "res=\" \"\n",
    "for index in dic_date[2022]:\n",
    "  \n",
    "  for el in index:\n",
    "\n",
    "    if(el in bag):\n",
    "      bag[el]+=1\n",
    "    else:\n",
    "      bag[el]=1\n",
    "    res=res+\" \"+str(el)\n",
    "print(res)\n",
    "words=res.split(' ')\n",
    "bigrams = zip(words, words[1:])\n",
    "bg=[]\n",
    "for w1 in bigrams:\n",
    "  bg.append(w1[0]+\"_\"+w1[1])\n",
    "\n",
    "counts = Counter(bg)\n",
    "resy={}\n",
    "sorted_keys2 = sorted(bag, key=bag.get, reverse=True) \n",
    "for w in sorted_keys2:\n",
    "    resy[w] = bag[w]\n",
    "sorted_dict1={}\n",
    "sorted_keys1 = sorted(counts, key=counts.get, reverse=True) \n",
    "for w in sorted_keys1:\n",
    "    sorted_dict1[w] = counts[w]\n",
    "print(resy)\n",
    "print(sorted_dict1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "True_clusteringAMG.ipynb",
   "provenance": [
    {
     "file_id": "1XPoc8LDFPKgx9o96jERygeF1tDkXT4Wy",
     "timestamp": 1.649873780884E12
    },
    {
     "file_id": "1hKWiD4hQwQamI9M1kQSvr8yUCFmaJtkn",
     "timestamp": 1.649033506928E12
    },
    {
     "file_id": "1yM3OqfbHFz71VPMLZ3UdSSvWXdESHAA-",
     "timestamp": 1.649031654591E12
    }
   ],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNlxQFgqWHEg/jLuK9Dq8H9"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
